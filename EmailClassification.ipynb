{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classififaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Required for BERT preprocessor or you can preprocess it urself\n",
    "import pandas as pd\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zm/87wl1bb92s16mph31dwf4s5c0000gn/T/ipykernel_9707/676033433.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('combined_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv('combined_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Processing step \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes loading the combined dataset, preprocessing text data, and preparing it for model training. There are columns for sender, receiver, date, subject, body, label, and urls. The label column appears to be a binary indicator where 1 might represent a phishing email, and the urls column indicates the presence of URLs in the email body, which is also marked as 1 for the presence of URLs. We want to preprocess both the subject and body as both of these fields carry significant information that can contribute to the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values\n",
    "data.fillna({'subject': '', 'body': '', 'label': 0}, inplace=True)\n",
    "\n",
    "# Handle 'urls' column: Create a binary flag indicating the presence of URLs\n",
    "data['urls_present'] = data['urls'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the CSV file, we need to do catergorise the data in a few ways:\n",
    "\n",
    "- Distribution of labels: Show the balance between phishing and non-phishing emails\n",
    "- Emails by date: How the voulme of emails varies over time\n",
    "- Presence of URLs in phishing vs. non-phishing emails: A comparison to see if phishing emails are more likely to contain URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distribution of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=data)\n",
    "plt.title('Distribution of Email Labels (Raw Data)')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-Phishing', 'Phishing'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Emails by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly convert datetimes to UTC\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce', utc=True)\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now().date()\n",
    "\n",
    "# First, ensure 'date' is in datetime format\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce', utc=True)\n",
    "\n",
    "# Extract the date part to focus on daily volume\n",
    "data['date_only'] = data['date'].dt.date\n",
    "\n",
    "# Count the number of emails per day\n",
    "emails_per_day = data.groupby('date_only').size()\n",
    "\n",
    "# Sort the counts by date\n",
    "emails_per_day_sorted = emails_per_day.sort_index()\n",
    "\n",
    "# Filter the dataset to exclude future dates\n",
    "emails_per_day_filtered = emails_per_day_sorted[emails_per_day_sorted.index <= current_date]\n",
    "\n",
    "# Plotting\n",
    "emails_per_day_filtered.plot(kind='line', figsize=(12, 6), marker='o', linestyle='-', logy=True)\n",
    "plt.title('Email Volume Per Day (Log Scale)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Emails (Log Scale)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display dates with the highest email volumes to identify outliers\n",
    "print(emails_per_day_sorted.sort_values(ascending=False).head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Presence of URLs in Phishing vs. Non-Phishing Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot using 'urls_present' instead of 'urls'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='label', hue='urls_present', data=data)\n",
    "plt.title('Presence of URLs in Phishing vs. Non-Phishing Emails')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=[0, 1], labels=['Non-Phishing', 'Phishing'])\n",
    "plt.legend(title='URLs Present', labels=['No', 'Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typicall, we should always split the data into training and testing sets before passing it through and preprocessing functions like the BERT preprocessor from Tensorflow Hub. This ensures that the preprocessing is done independtly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'subject' and 'body' headers into one column using a special token like [SEP]\n",
    "data['text'] = data['subject'] + \" [SEP] \" + data['body']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data['text'].values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder API: https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-l-12-h-768-a-12/versions/3 This SavedModel implements the encoder API for text embeddings with transformer encoders. It expects a dict with three int32 Tensors as input: input_word_ids, input_mask, and input_type_ids.\n",
    "\n",
    "- Preprocessor API: https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3 This SavedModel implements the preprocessor API for text embeddings with Transformer encoders, which offers several ways to go from one or more batches of text segments (plain text encoded as UTF-8) to the inputs for the Transformer encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input for the preprocessor\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT preprocessor and encoder from TensorFlow Hub\n",
    "preprocessor_url = \"https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3\"\n",
    "preprocessor = hub.KerasLayer(preprocessor_url)\n",
    "\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "\n",
    "encoder_url = \"https://kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-l-12-h-768-a-12/versions/3\"\n",
    "encoder = hub.KerasLayer(encoder_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pooled_output for classification tasks\n",
    "pooled_output = outputs['pooled_output']\n",
    "dropout = Dropout(0.1)(pooled_output)\n",
    "class_output = Dense(1, activation='sigmoid', name='class_output')(dropout)\n",
    "\n",
    "model = Model(inputs=[text_input], outputs=[class_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tft_m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
